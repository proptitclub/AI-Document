{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GoogLeNet.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCg3YJp09D-Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsQvPXJ09EvV",
        "colab_type": "text"
      },
      "source": [
        "#GOOGLENET:\n",
        "\n",
        "I) 1x1 Convolution và Network in Network:\n",
        "- Đối với AlexNet thì chỉ có những filter với kích cỡ 3x3, 5x5, 7x7 hay 11x11 thì ở đây chúng ta đcược giới thiệu 1 loại filter mới đó là 1x1 vậy nó để làm gì.\n",
        "- 1x1 Convolution được đưa vào nhằm điều chỉnh số chiều kênh qua đó có thể giảm đi lượng đáng kể lượng tính toán. Hãy xem hình sau đây:\n",
        "\n",
        "  ![alt text](https://miro.medium.com/max/844/1*s6_8m_0EpwrzZPunRGgFCQ.png)\n",
        "\n",
        "  - Nếu theo như hình trên lượng phép tính cần dùng là 5x5x480x14x14x48 = 112.9 triệu phép tính. nhưng nếu thêm vô 1x1 convolution \n",
        "\n",
        "  ![alt text](https://miro.medium.com/max/1366/1*SJDJyBGM__wRX9wJYIKv8w.png) \n",
        "  - lúc này sẽ có lượng phép tính cần dùng đó là 1x1x480x14x14x16 + 5x5x16x14x14x48 = 5.3 triệu phép tính bé hơn rất rất nhiều so với 112,9 triệu khi dùng chay filter 5x5.\n",
        "- Cách thức mà 1x1 convolution làm giảm số chiều ta có thể hình dung nó như 1 fully connected, có thể dễ hình dung hơn qua ảnh sau :\n",
        "  ![alt text](https://lh3.googleusercontent.com/proxy/XUBEWQSEcdORuDuMS9PEpxsdy5Q2svnNKo-1I6rR6wDo1lXTAcLYDek4X5CT0XkaLYh3VIs1_MdDulqsSe7gnWo4gBYQY50)\n",
        "  - qua đấy có thể thấy 1x1 convolution hoạt động như 1 fully connected \n",
        "  - Và qua đây có thể thấy 1 network nhỏ hay có thể nói đây là mô hình Network in Network.\n",
        "- không chỉ làm giảm đi lượng tính toán mà 1x1 convi=olution còn làm tăng tính bất tuyến tính cho mô hình và cũng giúp giảm thiểu việc bị overfiting\n",
        "\n",
        "II) Mô Hình Inception:\n",
        "\n",
        "- Đối với những mạng CNN trước đây như LeNet, AlexNet hay VGG chúng ta đều quan tâm nên chọn filter kích cỡ bao nhiêu để có thể đạt kết quả cao nhất và tạo ra mô hình có chiều sâu nhât. Điều này là quan trong vói những mô hình mạng nói trên. Tuy nhiều thay vì đắn đo nên chọn cái nào thì thay vào đó ta sẽ chọn tất cả các filter đó và kết hợp những feature map con thành 1 feature map lớn hơn.\n",
        "\n",
        "- Mô hình Inception sẽ áp dụng việc dùng nhiều filter để mỗi kiểu filter sẽ tạo ra một feature map riêng và cuối cùng sẽ ghép nối những feature map ấy thành 1 feature map mới:\n",
        "\n",
        "  ![alt text](https://miro.medium.com/max/1098/1*m1wn5P5BFZydFgVd3RiZNw.png)\n",
        "\n",
        "- Nhưng lại 1 vấn đề ở đây đó là nếu áp nguyên cấu trúc ở trên thì có thể thấy chiều xâu của feature map tăng rất nhanh chóng và khiến khối lượng tính toán tăng lên rất nhiều. Do đó ta sẽ thêm các lớp 1x1 Convolution với mục đích giảm tải chiều xâu và ta có Câu trúc như sau:\n",
        "\n",
        "  ![alt text](https://miro.medium.com/max/1108/1*sezFsYW1MyM9YOMa1q909A.png)\n",
        "\n",
        "III) Mô hình GoogLeNet :\n",
        "- Đó là mô hình gồm nhiều các khối inception và sẽ gồm có 22 lớp( chỉ những lớp có trọng số và bỏ qua các lớp song song, tương đương nhau)\n",
        "\n",
        "![alt text](https://miro.medium.com/max/1400/1*lRN3h9a_qJdT6NIy0VOu3Q.png)\n",
        "\n",
        "- với 22 lớp thì chiruf sâu của mô hình đã vượt lên so với nhưng mô hình đi trc như AlexNet và VGG( nhưng vẫn kém xa so với ResNet được phát triển sau đó 1 năm)\n",
        "- ở đây ta sẽ dùng những khối cấu trúc Inception gộp lại với nhau và ở layer gần cuối ta sử dụng Globe Average Pooling ( GAP ) đây là 1 cách rất hay để giảm tải lượng tính toán so với áp dụng nguyên Fully connected thông thường sẽ tốn rất nhiều phép tính từ đó tăng tính chính xác cũng như giảm bớt việc bị over fiting.\n",
        "\n",
        "![alt text](https://miro.medium.com/max/1400/1*0-wMHcASLDFzx9YBRCZXHg.png)\n",
        "\n",
        "- Nhưng mạng quá sâu lại dính tới 1 vấn đề khác đó là biến mất đạo hàm ( vanished gradient) điều này có thể khiến cho mô hình của chúng ta sẽ có phần dần dần chết cho không còn được cập nhật bằng gradient nữa như vậy sẽ rất phí phạm. Do đó GoogLeNet đã có cách giải quyết đó là với những lớp gần cuối sẽ thêm những nhánh mới được gọi là auxiliary classifiers những lớp này hoạt động như phần FCs để đưa ra dự đoán và tính error từ đó đưa ra vieccj tinh chỉnh lại Layer trước đó qua quá trình back-propagation sao cho hội tụ trước khi đi đến với layer tiếp theo. Điều này khiến cho việc phân tích dữ liệu tốt hơn và cũng chống lại việc vanished gradient hiệu quả hơn.\n",
        "\n"
      ]
    }
  ]
}